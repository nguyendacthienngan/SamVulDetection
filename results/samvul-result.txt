Epoch 1:
Epoch 1/10, Average Training Loss: 0.7381
Validation Accuracy: 0.9484, Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000
Epoch 1/10, Validation Loss: 0.0000
===
Epoch 7:
Epoch 7/10, Average Training Loss: 0.5669
Validation Accuracy: 0.8070, Precision: 0.1458, Recall: 0.5635, F1-Score: 0.2317

===
Epoch 9:
Epoch 9/10, Average Loss: 0.4898
Predictions distribution: {0: 29651, 1: 21281}
Validation Accuracy: 0.6169, Precision: 0.1033, Recall: 0.8361, F1-Score: 0.1839What do you think about this progress?

Epoch 10/10, Average Loss: 0.4421
Model saved to /home/ngan/Documents/SamVulDetection/saved_models/model_epoch_10.pth
Predictions distribution: {0: 37688, 1: 13244}
Validation Accuracy: 0.7604, Precision: 0.1386, Recall: 0.6981, F1-Score: 0.2313

Epoch 10: Learning rate: [1.0000000000000002e-06]

Epoch 1/10, Average Loss: 0.3992
Model saved to /home/ngan/Documents/SamVulDetection/saved_models/CombinedModel_epoch_1.pth
Predictions distribution: {0: 37613, 1: 13319}
Validation Accuracy: 0.7592, Precision: 0.1384, Recall: 0.7008, F1-Score: 0.2311
Epoch 1/10, Validation Loss: 0.4936
Epoch 1: Learning rate: [1e-05]
Best model saved to /home/ngan/Documents/SamVulDetection/saved_models/best_model.pth with validation loss: 0.4936
Epoch: 1
batch_idx: 0

Epoch: 10
batch_idx: 0

/home/ngan/.local/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  assert input.numel() == input.storage().size(), (

Epoch [11/10], Step [1/944], Loss: 0.3100
batch_idx: 1
batch_idx: 2
batch_idx: 3
batch_idx: 4
batch_idx: 5
batch_idx: 6
batch_idx: 7
batch_idx: 8
batch_idx: 9
batch_idx: 10
Epoch [11/10], Step [11/944], Loss: 0.5510
batch_idx: 11
batch_idx: 12
batch_idx: 13
batch_idx: 14
batch_idx: 15
batch_idx: 16
batch_idx: 17
batch_idx: 18
batch_idx: 19
batch_idx: 20
Epoch [11/10], Step [21/944], Loss: 0.3926
batch_idx: 21
batch_idx: 22
batch_idx: 23
batch_idx: 24
batch_idx: 25
batch_idx: 26
batch_idx: 27
batch_idx: 28
batch_idx: 29
batch_idx: 30
Epoch [11/10], Step [31/944], Loss: 0.5817
batch_idx: 31
batch_idx: 32
batch_idx: 33
batch_idx: 34
batch_idx: 35
batch_idx: 36
batch_idx: 37
batch_idx: 38
batch_idx: 39
batch_idx: 40
Epoch [11/10], Step [41/944], Loss: 0.3726
batch_idx: 41
batch_idx: 42
batch_idx: 43
batch_idx: 44
batch_idx: 45
batch_idx: 46
batch_idx: 47
batch_idx: 48
batch_idx: 49
batch_idx: 50
Epoch [11/10], Step [51/944], Loss: 0.3950
batch_idx: 51
batch_idx: 52
batch_idx: 53
batch_idx: 54
batch_idx: 55
batch_idx: 56
batch_idx: 57
batch_idx: 58
batch_idx: 59
batch_idx: 60
Epoch [11/10], Step [61/944], Loss: 0.3312
batch_idx: 61
batch_idx: 62
batch_idx: 63
batch_idx: 64
batch_idx: 65
batch_idx: 66
batch_idx: 67
batch_idx: 68
batch_idx: 69
batch_idx: 70
Epoch [11/10], Step [71/944], Loss: 0.3004
batch_idx: 71
batch_idx: 72
batch_idx: 73
batch_idx: 74
batch_idx: 75
batch_idx: 76
batch_idx: 77
batch_idx: 78
batch_idx: 79
batch_idx: 80
Epoch [11/10], Step [81/944], Loss: 0.4920
batch_idx: 81
batch_idx: 82
batch_idx: 83
batch_idx: 84
batch_idx: 85
batch_idx: 86
batch_idx: 87
batch_idx: 88
batch_idx: 89
batch_idx: 90
Epoch [11/10], Step [91/944], Loss: 0.6627
batch_idx: 91
batch_idx: 92
batch_idx: 93
batch_idx: 94
batch_idx: 95
batch_idx: 96
batch_idx: 97
batch_idx: 98
batch_idx: 99
batch_idx: 100
Epoch [11/10], Step [101/944], Loss: 0.3754
batch_idx: 101
batch_idx: 102
batch_idx: 103
batch_idx: 104
batch_idx: 105
batch_idx: 106
batch_idx: 107
batch_idx: 108
batch_idx: 109
batch_idx: 110
Epoch [11/10], Step [111/944], Loss: 0.3620
batch_idx: 111
batch_idx: 112
batch_idx: 113
batch_idx: 114
batch_idx: 115
batch_idx: 116
batch_idx: 117
batch_idx: 118
batch_idx: 119
batch_idx: 120
Epoch [11/10], Step [121/944], Loss: 0.4026
batch_idx: 121
batch_idx: 122
batch_idx: 123
batch_idx: 124
batch_idx: 125
batch_idx: 126
batch_idx: 127
batch_idx: 128
batch_idx: 129
batch_idx: 130
Epoch [11/10], Step [131/944], Loss: 0.3646
batch_idx: 131
batch_idx: 132
batch_idx: 133
batch_idx: 134
batch_idx: 135
batch_idx: 136
batch_idx: 137
batch_idx: 138
batch_idx: 139
batch_idx: 140
Epoch [11/10], Step [141/944], Loss: 0.3897
batch_idx: 141
batch_idx: 142
batch_idx: 143
batch_idx: 144
batch_idx: 145
batch_idx: 146
batch_idx: 147
batch_idx: 148
batch_idx: 149
batch_idx: 150
Epoch [11/10], Step [151/944], Loss: 0.2721
batch_idx: 151
batch_idx: 152
batch_idx: 153
batch_idx: 154
batch_idx: 155
batch_idx: 156
batch_idx: 157
batch_idx: 158
batch_idx: 159
batch_idx: 160
Epoch [11/10], Step [161/944], Loss: 0.4943
batch_idx: 161
batch_idx: 162
batch_idx: 163
batch_idx: 164
batch_idx: 165
batch_idx: 166
batch_idx: 167
batch_idx: 168
batch_idx: 169
batch_idx: 170
Epoch [11/10], Step [171/944], Loss: 0.5325
batch_idx: 171
batch_idx: 172
batch_idx: 173
batch_idx: 174
batch_idx: 175
batch_idx: 176
batch_idx: 177
batch_idx: 178
batch_idx: 179
batch_idx: 180
Epoch [11/10], Step [181/944], Loss: 0.4401
batch_idx: 181
batch_idx: 182
batch_idx: 183
batch_idx: 184
batch_idx: 185
batch_idx: 186
batch_idx: 187
batch_idx: 188
batch_idx: 189
batch_idx: 190
Epoch [11/10], Step [191/944], Loss: 0.3707
batch_idx: 191
batch_idx: 192
batch_idx: 193
batch_idx: 194
batch_idx: 195
batch_idx: 196
batch_idx: 197
batch_idx: 198
batch_idx: 199
batch_idx: 200
Epoch [11/10], Step [201/944], Loss: 0.3343
batch_idx: 201
batch_idx: 202
batch_idx: 203
batch_idx: 204
batch_idx: 205
batch_idx: 206
batch_idx: 207
batch_idx: 208
batch_idx: 209
batch_idx: 210
Epoch [11/10], Step [211/944], Loss: 0.5143
batch_idx: 211
batch_idx: 212
batch_idx: 213
batch_idx: 214
batch_idx: 215
batch_idx: 216
batch_idx: 217
batch_idx: 218
batch_idx: 219
batch_idx: 220
Epoch [11/10], Step [221/944], Loss: 0.4606
batch_idx: 221
batch_idx: 222
batch_idx: 223
batch_idx: 224
batch_idx: 225
batch_idx: 226
batch_idx: 227
batch_idx: 228
batch_idx: 229
batch_idx: 230
Epoch [11/10], Step [231/944], Loss: 0.3976
batch_idx: 231
batch_idx: 232
batch_idx: 233
batch_idx: 234
batch_idx: 235
batch_idx: 236
batch_idx: 237
batch_idx: 238
batch_idx: 239
batch_idx: 240
Epoch [11/10], Step [241/944], Loss: 0.4944
batch_idx: 241
batch_idx: 242
batch_idx: 243
batch_idx: 244
batch_idx: 245
batch_idx: 246
batch_idx: 247
batch_idx: 248
batch_idx: 249
batch_idx: 250
Epoch [11/10], Step [251/944], Loss: 0.3399
batch_idx: 251
batch_idx: 252
batch_idx: 253
batch_idx: 254
batch_idx: 255
batch_idx: 256
batch_idx: 257
batch_idx: 258
batch_idx: 259
batch_idx: 260
Epoch [11/10], Step [261/944], Loss: 0.3320
batch_idx: 261
batch_idx: 262
batch_idx: 263
batch_idx: 264
batch_idx: 265
batch_idx: 266
batch_idx: 267
batch_idx: 268
batch_idx: 269
batch_idx: 270
Epoch [11/10], Step [271/944], Loss: 0.3412
batch_idx: 271
batch_idx: 272
batch_idx: 273
batch_idx: 274
batch_idx: 275
batch_idx: 276
batch_idx: 277
batch_idx: 278
batch_idx: 279
batch_idx: 280
Epoch [11/10], Step [281/944], Loss: 0.5285
batch_idx: 281
batch_idx: 282
batch_idx: 283
batch_idx: 284
batch_idx: 285
batch_idx: 286
batch_idx: 287
batch_idx: 288
batch_idx: 289
batch_idx: 290
Epoch [11/10], Step [291/944], Loss: 0.4180
batch_idx: 291
batch_idx: 292
batch_idx: 293
batch_idx: 294
batch_idx: 295
batch_idx: 296
batch_idx: 297
batch_idx: 298
batch_idx: 299
batch_idx: 300
Epoch [11/10], Step [301/944], Loss: 0.4498
batch_idx: 301
batch_idx: 302
batch_idx: 303
batch_idx: 304
batch_idx: 305
batch_idx: 306
batch_idx: 307
batch_idx: 308
batch_idx: 309
batch_idx: 310
Epoch [11/10], Step [311/944], Loss: 0.4522
batch_idx: 311
batch_idx: 312
batch_idx: 313
batch_idx: 314
batch_idx: 315
batch_idx: 316
batch_idx: 317
batch_idx: 318
batch_idx: 319
batch_idx: 320
Epoch [11/10], Step [321/944], Loss: 0.3504
batch_idx: 321
batch_idx: 322
batch_idx: 323
batch_idx: 324
batch_idx: 325
batch_idx: 326
batch_idx: 327
batch_idx: 328
batch_idx: 329
batch_idx: 330
Epoch [11/10], Step [331/944], Loss: 0.3946
batch_idx: 331
batch_idx: 332
batch_idx: 333
batch_idx: 334
batch_idx: 335
batch_idx: 336
batch_idx: 337
batch_idx: 338
batch_idx: 339
batch_idx: 340
Epoch [11/10], Step [341/944], Loss: 0.3704
batch_idx: 341
batch_idx: 342
batch_idx: 343
batch_idx: 344
batch_idx: 345
batch_idx: 346
batch_idx: 347
batch_idx: 348
batch_idx: 349
batch_idx: 350
Epoch [11/10], Step [351/944], Loss: 0.6361
batch_idx: 351
batch_idx: 352
batch_idx: 353
batch_idx: 354
batch_idx: 355
batch_idx: 356
batch_idx: 357
batch_idx: 358
batch_idx: 359
batch_idx: 360
Epoch [11/10], Step [361/944], Loss: 0.4212
batch_idx: 361
batch_idx: 362
batch_idx: 363
batch_idx: 364
batch_idx: 365
batch_idx: 366
batch_idx: 367
batch_idx: 368
batch_idx: 369
batch_idx: 370
Epoch [11/10], Step [371/944], Loss: 0.4459
batch_idx: 371
batch_idx: 372
batch_idx: 373
batch_idx: 374
batch_idx: 375
batch_idx: 376
batch_idx: 377
batch_idx: 378
batch_idx: 379
batch_idx: 380
Epoch [11/10], Step [381/944], Loss: 0.4815
batch_idx: 381
batch_idx: 382
batch_idx: 383
batch_idx: 384
batch_idx: 385
batch_idx: 386
batch_idx: 387
batch_idx: 388
batch_idx: 389
batch_idx: 390
Epoch [11/10], Step [391/944], Loss: 0.4670
batch_idx: 391
batch_idx: 392
batch_idx: 393
batch_idx: 394
batch_idx: 395
batch_idx: 396
batch_idx: 397
batch_idx: 398
batch_idx: 399
batch_idx: 400
Epoch [11/10], Step [401/944], Loss: 0.4654
batch_idx: 401
batch_idx: 402
batch_idx: 403
batch_idx: 404
batch_idx: 405
batch_idx: 406
batch_idx: 407
batch_idx: 408
batch_idx: 409
batch_idx: 410
Epoch [11/10], Step [411/944], Loss: 0.3476
batch_idx: 411
batch_idx: 412
batch_idx: 413
batch_idx: 414
batch_idx: 415
batch_idx: 416
batch_idx: 417
batch_idx: 418
batch_idx: 419
batch_idx: 420
Epoch [11/10], Step [421/944], Loss: 0.4462
batch_idx: 421
batch_idx: 422
batch_idx: 423
batch_idx: 424
batch_idx: 425
batch_idx: 426
batch_idx: 427
batch_idx: 428
batch_idx: 429
batch_idx: 430
Epoch [11/10], Step [431/944], Loss: 0.3448
batch_idx: 431
batch_idx: 432
batch_idx: 433
batch_idx: 434
batch_idx: 435
batch_idx: 436
batch_idx: 437
batch_idx: 438
batch_idx: 439
batch_idx: 440
Epoch [11/10], Step [441/944], Loss: 0.4123
batch_idx: 441
batch_idx: 442
batch_idx: 443
batch_idx: 444
batch_idx: 445
batch_idx: 446
batch_idx: 447
batch_idx: 448
batch_idx: 449
batch_idx: 450
Epoch [11/10], Step [451/944], Loss: 0.4973
batch_idx: 451
batch_idx: 452
batch_idx: 453
batch_idx: 454
batch_idx: 455
batch_idx: 456
batch_idx: 457
batch_idx: 458
batch_idx: 459
batch_idx: 460
Epoch [11/10], Step [461/944], Loss: 0.2770
batch_idx: 461
batch_idx: 462
batch_idx: 463
batch_idx: 464
batch_idx: 465
batch_idx: 466
batch_idx: 467
batch_idx: 468
batch_idx: 469
batch_idx: 470
Epoch [11/10], Step [471/944], Loss: 0.5414
batch_idx: 471
batch_idx: 472
batch_idx: 473
batch_idx: 474
batch_idx: 475
batch_idx: 476
batch_idx: 477
batch_idx: 478
batch_idx: 479
batch_idx: 480
Epoch [11/10], Step [481/944], Loss: 0.4387
batch_idx: 481
batch_idx: 482
batch_idx: 483
batch_idx: 484
batch_idx: 485
batch_idx: 486
batch_idx: 487
batch_idx: 488
batch_idx: 489
batch_idx: 490
Epoch [11/10], Step [491/944], Loss: 0.4254
batch_idx: 491
batch_idx: 492
batch_idx: 493
batch_idx: 494
batch_idx: 495
batch_idx: 496
batch_idx: 497
batch_idx: 498
batch_idx: 499
batch_idx: 500
Epoch [11/10], Step [501/944], Loss: 0.3940
batch_idx: 501
batch_idx: 502
batch_idx: 503
batch_idx: 504
batch_idx: 505
batch_idx: 506
batch_idx: 507
batch_idx: 508
batch_idx: 509
batch_idx: 510
Epoch [11/10], Step [511/944], Loss: 0.4936
batch_idx: 511
batch_idx: 512
batch_idx: 513
batch_idx: 514
batch_idx: 515
batch_idx: 516
batch_idx: 517
batch_idx: 518
batch_idx: 519
batch_idx: 520
Epoch [11/10], Step [521/944], Loss: 0.4248
batch_idx: 521
batch_idx: 522
batch_idx: 523
batch_idx: 524
batch_idx: 525
batch_idx: 526
batch_idx: 527
batch_idx: 528
batch_idx: 529
batch_idx: 530
Epoch [11/10], Step [531/944], Loss: 0.2375
batch_idx: 531
batch_idx: 532
batch_idx: 533
batch_idx: 534
batch_idx: 535
batch_idx: 536
batch_idx: 537
batch_idx: 538
batch_idx: 539
batch_idx: 540
Epoch [11/10], Step [541/944], Loss: 0.3564
batch_idx: 541
batch_idx: 542
batch_idx: 543
batch_idx: 544
batch_idx: 545
batch_idx: 546
batch_idx: 547
batch_idx: 548
batch_idx: 549
batch_idx: 550
Epoch [11/10], Step [551/944], Loss: 0.4098
batch_idx: 551
batch_idx: 552
batch_idx: 553
batch_idx: 554
batch_idx: 555
batch_idx: 556
batch_idx: 557
batch_idx: 558
batch_idx: 559
batch_idx: 560
Epoch [11/10], Step [561/944], Loss: 0.4067
batch_idx: 561
batch_idx: 562
batch_idx: 563
batch_idx: 564
batch_idx: 565
batch_idx: 566
batch_idx: 567
batch_idx: 568
batch_idx: 569
batch_idx: 570
Epoch [11/10], Step [571/944], Loss: 0.3434
batch_idx: 571
batch_idx: 572
batch_idx: 573
batch_idx: 574
batch_idx: 575
batch_idx: 576
batch_idx: 577
batch_idx: 578
batch_idx: 579
batch_idx: 580
Epoch [11/10], Step [581/944], Loss: 0.3424
batch_idx: 581
batch_idx: 582
batch_idx: 583
batch_idx: 584
batch_idx: 585
batch_idx: 586
batch_idx: 587
batch_idx: 588
batch_idx: 589
batch_idx: 590
Epoch [11/10], Step [591/944], Loss: 0.5038
batch_idx: 591
batch_idx: 592
batch_idx: 593
batch_idx: 594
batch_idx: 595
batch_idx: 596
batch_idx: 597
batch_idx: 598
batch_idx: 599
batch_idx: 600
Epoch [11/10], Step [601/944], Loss: 0.3996
batch_idx: 601
batch_idx: 602
batch_idx: 603
batch_idx: 604
batch_idx: 605
batch_idx: 606
batch_idx: 607
batch_idx: 608
batch_idx: 609
batch_idx: 610
Epoch [11/10], Step [611/944], Loss: 0.3502
batch_idx: 611
batch_idx: 612
batch_idx: 613
batch_idx: 614
batch_idx: 615
batch_idx: 616
batch_idx: 617
batch_idx: 618
batch_idx: 619
batch_idx: 620
Epoch [11/10], Step [621/944], Loss: 0.5129
batch_idx: 621
batch_idx: 622
batch_idx: 623
batch_idx: 624
batch_idx: 625
batch_idx: 626
batch_idx: 627
batch_idx: 628
batch_idx: 629
batch_idx: 630
Epoch [11/10], Step [631/944], Loss: 0.4100
batch_idx: 631
batch_idx: 632
batch_idx: 633
batch_idx: 634
batch_idx: 635
batch_idx: 636
batch_idx: 637
batch_idx: 638
batch_idx: 639
batch_idx: 640
Epoch [11/10], Step [641/944], Loss: 0.2595
batch_idx: 641
batch_idx: 642
batch_idx: 643
batch_idx: 644
batch_idx: 645
batch_idx: 646
batch_idx: 647
batch_idx: 648
batch_idx: 649
batch_idx: 650
Epoch [11/10], Step [651/944], Loss: 0.2811
batch_idx: 651
batch_idx: 652
batch_idx: 653
batch_idx: 654
batch_idx: 655
batch_idx: 656
batch_idx: 657
batch_idx: 658
batch_idx: 659
batch_idx: 660
Epoch [11/10], Step [661/944], Loss: 0.3725
batch_idx: 661
batch_idx: 662
batch_idx: 663
batch_idx: 664
batch_idx: 665
batch_idx: 666
batch_idx: 667
batch_idx: 668
batch_idx: 669
batch_idx: 670
Epoch [11/10], Step [671/944], Loss: 0.4149
batch_idx: 671
batch_idx: 672
batch_idx: 673
batch_idx: 674
batch_idx: 675
batch_idx: 676
batch_idx: 677
batch_idx: 678
batch_idx: 679
batch_idx: 680
Epoch [11/10], Step [681/944], Loss: 0.3444
batch_idx: 681
batch_idx: 682
batch_idx: 683
batch_idx: 684
batch_idx: 685
batch_idx: 686
batch_idx: 687
batch_idx: 688
batch_idx: 689
batch_idx: 690
Epoch [11/10], Step [691/944], Loss: 0.3758
batch_idx: 691
batch_idx: 692
batch_idx: 693
batch_idx: 694
batch_idx: 695
batch_idx: 696
batch_idx: 697
batch_idx: 698
batch_idx: 699
batch_idx: 700
Epoch [11/10], Step [701/944], Loss: 0.3641
batch_idx: 701
batch_idx: 702
batch_idx: 703
batch_idx: 704
batch_idx: 705
batch_idx: 706
batch_idx: 707
batch_idx: 708
batch_idx: 709
batch_idx: 710
Epoch [11/10], Step [711/944], Loss: 0.4627
batch_idx: 711
batch_idx: 712
batch_idx: 713
batch_idx: 714
batch_idx: 715
batch_idx: 716
batch_idx: 717
batch_idx: 718
batch_idx: 719
batch_idx: 720
Epoch [11/10], Step [721/944], Loss: 0.3558
batch_idx: 721
batch_idx: 722
batch_idx: 723
batch_idx: 724
batch_idx: 725
batch_idx: 726
batch_idx: 727
batch_idx: 728
batch_idx: 729
batch_idx: 730
Epoch [11/10], Step [731/944], Loss: 0.5057
batch_idx: 731
batch_idx: 732
batch_idx: 733
batch_idx: 734
batch_idx: 735
batch_idx: 736
batch_idx: 737
batch_idx: 738
batch_idx: 739
batch_idx: 740
Epoch [11/10], Step [741/944], Loss: 0.5337
batch_idx: 741
batch_idx: 742
batch_idx: 743
batch_idx: 744
batch_idx: 745
batch_idx: 746
batch_idx: 747
batch_idx: 748
batch_idx: 749
batch_idx: 750
Epoch [11/10], Step [751/944], Loss: 0.4165
batch_idx: 751
batch_idx: 752
batch_idx: 753
batch_idx: 754
batch_idx: 755
batch_idx: 756
batch_idx: 757
batch_idx: 758
batch_idx: 759
batch_idx: 760
Epoch [11/10], Step [761/944], Loss: 0.4412
batch_idx: 761
batch_idx: 762
batch_idx: 763
batch_idx: 764
batch_idx: 765
batch_idx: 766
batch_idx: 767
batch_idx: 768
batch_idx: 769
batch_idx: 770
Epoch [11/10], Step [771/944], Loss: 0.3679
batch_idx: 771
batch_idx: 772
batch_idx: 773
batch_idx: 774
batch_idx: 775
batch_idx: 776
batch_idx: 777
batch_idx: 778
batch_idx: 779
batch_idx: 780
Epoch [11/10], Step [781/944], Loss: 0.4029
batch_idx: 781
batch_idx: 782
batch_idx: 783
batch_idx: 784
batch_idx: 785
batch_idx: 786
batch_idx: 787
batch_idx: 788
batch_idx: 789
batch_idx: 790
Epoch [11/10], Step [791/944], Loss: 0.4127
batch_idx: 791
batch_idx: 792
batch_idx: 793
batch_idx: 794
batch_idx: 795
batch_idx: 796
batch_idx: 797
batch_idx: 798
batch_idx: 799
batch_idx: 800
Epoch [11/10], Step [801/944], Loss: 0.3353
batch_idx: 801
batch_idx: 802
batch_idx: 803
batch_idx: 804
batch_idx: 805
batch_idx: 806
batch_idx: 807
batch_idx: 808
batch_idx: 809
batch_idx: 810
Epoch [11/10], Step [811/944], Loss: 0.2518
batch_idx: 811
batch_idx: 812
batch_idx: 813
batch_idx: 814
batch_idx: 815
batch_idx: 816
batch_idx: 817
batch_idx: 818
batch_idx: 819
batch_idx: 820
Epoch [11/10], Step [821/944], Loss: 0.6384
batch_idx: 821
batch_idx: 822
batch_idx: 823
batch_idx: 824
batch_idx: 825
batch_idx: 826
batch_idx: 827
batch_idx: 828
batch_idx: 829
batch_idx: 830
Epoch [11/10], Step [831/944], Loss: 0.4496
batch_idx: 831
batch_idx: 832
batch_idx: 833
batch_idx: 834
batch_idx: 835
batch_idx: 836
batch_idx: 837
batch_idx: 838
batch_idx: 839
batch_idx: 840
Epoch [11/10], Step [841/944], Loss: 0.3354
batch_idx: 841
batch_idx: 842
batch_idx: 843
batch_idx: 844
batch_idx: 845
batch_idx: 846
batch_idx: 847
batch_idx: 848
batch_idx: 849
batch_idx: 850
Epoch [11/10], Step [851/944], Loss: 0.4292
batch_idx: 851
batch_idx: 852
batch_idx: 853
batch_idx: 854
batch_idx: 855
batch_idx: 856
batch_idx: 857
batch_idx: 858
batch_idx: 859
batch_idx: 860
Epoch [11/10], Step [861/944], Loss: 0.5524
batch_idx: 861
batch_idx: 862
batch_idx: 863
batch_idx: 864
batch_idx: 865
batch_idx: 866
batch_idx: 867
batch_idx: 868
batch_idx: 869
batch_idx: 870
Epoch [11/10], Step [871/944], Loss: 0.3566
batch_idx: 871
batch_idx: 872
batch_idx: 873
batch_idx: 874
batch_idx: 875
batch_idx: 876
batch_idx: 877
batch_idx: 878
batch_idx: 879
batch_idx: 880
Epoch [11/10], Step [881/944], Loss: 0.3098
batch_idx: 881
batch_idx: 882
batch_idx: 883
batch_idx: 884
batch_idx: 885
batch_idx: 886
batch_idx: 887
batch_idx: 888
batch_idx: 889
batch_idx: 890
Epoch [11/10], Step [891/944], Loss: 0.3757
batch_idx: 891
batch_idx: 892
batch_idx: 893
batch_idx: 894
batch_idx: 895
batch_idx: 896
batch_idx: 897
batch_idx: 898
batch_idx: 899
batch_idx: 900
Epoch [11/10], Step [901/944], Loss: 0.3844
batch_idx: 901
batch_idx: 902
batch_idx: 903
batch_idx: 904
batch_idx: 905
batch_idx: 906
batch_idx: 907
batch_idx: 908
batch_idx: 909
batch_idx: 910
Epoch [11/10], Step [911/944], Loss: 0.4800
batch_idx: 911
batch_idx: 912
batch_idx: 913
batch_idx: 914
batch_idx: 915
batch_idx: 916
batch_idx: 917
batch_idx: 918
batch_idx: 919
batch_idx: 920
Epoch [11/10], Step [921/944], Loss: 0.4616
batch_idx: 921
batch_idx: 922
batch_idx: 923
batch_idx: 924
batch_idx: 925
batch_idx: 926
batch_idx: 927
batch_idx: 928
batch_idx: 929
batch_idx: 930
Epoch [11/10], Step [931/944], Loss: 0.5083
batch_idx: 931
batch_idx: 932
batch_idx: 933
batch_idx: 934
batch_idx: 935
batch_idx: 936
batch_idx: 937
batch_idx: 938
batch_idx: 939
batch_idx: 940
Epoch [11/10], Step [941/944], Loss: 0.2638
batch_idx: 941
batch_idx: 942
batch_idx: 943
Gradients for clr_model.model.roberta.embeddings.word_embeddings.weight: None
Gradients for clr_model.model.roberta.embeddings.position_embeddings.weight: None
Gradients for clr_model.model.roberta.embeddings.token_type_embeddings.weight: None
Gradients for clr_model.model.roberta.embeddings.LayerNorm.weight: None
Gradients for clr_model.model.roberta.embeddings.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.LayerNorm.bias: None
Gradients for clr_model.model.classifier.dense.weight: None
Gradients for clr_model.model.classifier.dense.bias: None
Gradients for clr_model.model.classifier.out_proj.weight: None
Gradients for clr_model.model.classifier.out_proj.bias: None
Gradients for devign_model.ggnn.linears.0.weight: 0.0
Gradients for devign_model.ggnn.linears.0.bias: 0.0
Gradients for devign_model.ggnn.linears.1.weight: 0.0
Gradients for devign_model.ggnn.linears.1.bias: 0.0
Gradients for devign_model.ggnn.linears.2.weight: 0.0
Gradients for devign_model.ggnn.linears.2.bias: 0.0
Gradients for devign_model.ggnn.linears.3.weight: 0.0
Gradients for devign_model.ggnn.linears.3.bias: 0.0
Gradients for devign_model.ggnn.linears.4.weight: 0.0
Gradients for devign_model.ggnn.linears.4.bias: 0.0
Gradients for devign_model.ggnn.linears.5.weight: 0.0
Gradients for devign_model.ggnn.linears.5.bias: 0.0
Gradients for devign_model.ggnn.gru.weight_ih: 0.0
Gradients for devign_model.ggnn.gru.weight_hh: 0.0
Gradients for devign_model.ggnn.gru.bias_ih: 0.0
Gradients for devign_model.ggnn.gru.bias_hh: 0.0
Gradients for devign_model.conv_l1.weight: 0.0
Gradients for devign_model.conv_l1.bias: 0.0
Gradients for devign_model.conv_l2.weight: 0.0
Gradients for devign_model.conv_l2.bias: 0.0
Gradients for devign_model.conv_l1_for_concat.weight: 0.0
Gradients for devign_model.conv_l1_for_concat.bias: 0.0
Gradients for devign_model.conv_l2_for_concat.weight: 0.0
Gradients for devign_model.conv_l2_for_concat.bias: 0.0
Gradients for devign_model.mlp_z.weight: 0.0
Gradients for devign_model.mlp_z.bias: 0.0
Gradients for devign_model.mlp_y.weight: 0.0
Gradients for devign_model.mlp_y.bias: 0.0
Gradients for classifier.weight: 4.470348358154297e-08
Gradients for classifier.bias: 0.0
Epoch 11/10, Average Loss: 0.4013
Model saved to /home/ngan/Documents/SamVulDetection/saved_models/model_epoch_11.pth


Epoch: 10
batch_idx: 0

/home/ngan/.local/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  assert input.numel() == input.storage().size(), (

Epoch [11/10], Step [1/944], Loss: 0.3100
batch_idx: 1
batch_idx: 2
batch_idx: 3
batch_idx: 4
batch_idx: 5
batch_idx: 6
batch_idx: 7
batch_idx: 8
batch_idx: 9
batch_idx: 10
Epoch [11/10], Step [11/944], Loss: 0.5510
batch_idx: 11
batch_idx: 12
batch_idx: 13
batch_idx: 14
batch_idx: 15
batch_idx: 16
batch_idx: 17
batch_idx: 18
batch_idx: 19
batch_idx: 20
Epoch [11/10], Step [21/944], Loss: 0.3926
batch_idx: 21
batch_idx: 22
batch_idx: 23
batch_idx: 24
batch_idx: 25
batch_idx: 26
batch_idx: 27
batch_idx: 28
batch_idx: 29
batch_idx: 30
Epoch [11/10], Step [31/944], Loss: 0.5817
batch_idx: 31
batch_idx: 32
batch_idx: 33
batch_idx: 34
batch_idx: 35
batch_idx: 36
batch_idx: 37
batch_idx: 38
batch_idx: 39
batch_idx: 40
Epoch [11/10], Step [41/944], Loss: 0.3726
batch_idx: 41
batch_idx: 42
batch_idx: 43
batch_idx: 44
batch_idx: 45
batch_idx: 46
batch_idx: 47
batch_idx: 48
batch_idx: 49
batch_idx: 50
Epoch [11/10], Step [51/944], Loss: 0.3950
batch_idx: 51
batch_idx: 52
batch_idx: 53
batch_idx: 54
batch_idx: 55
batch_idx: 56
batch_idx: 57
batch_idx: 58
batch_idx: 59
batch_idx: 60
Epoch [11/10], Step [61/944], Loss: 0.3312
batch_idx: 61
batch_idx: 62
batch_idx: 63
batch_idx: 64
batch_idx: 65
batch_idx: 66
batch_idx: 67
batch_idx: 68
batch_idx: 69
batch_idx: 70
Epoch [11/10], Step [71/944], Loss: 0.3004
batch_idx: 71
batch_idx: 72
batch_idx: 73
batch_idx: 74
batch_idx: 75
batch_idx: 76
batch_idx: 77
batch_idx: 78
batch_idx: 79
batch_idx: 80
Epoch [11/10], Step [81/944], Loss: 0.4920
batch_idx: 81
batch_idx: 82
batch_idx: 83
batch_idx: 84
batch_idx: 85
batch_idx: 86
batch_idx: 87
batch_idx: 88
batch_idx: 89
batch_idx: 90
Epoch [11/10], Step [91/944], Loss: 0.6627
batch_idx: 91
batch_idx: 92
batch_idx: 93
batch_idx: 94
batch_idx: 95
batch_idx: 96
batch_idx: 97
batch_idx: 98
batch_idx: 99
batch_idx: 100
Epoch [11/10], Step [101/944], Loss: 0.3754
batch_idx: 101
batch_idx: 102
batch_idx: 103
batch_idx: 104
batch_idx: 105
batch_idx: 106
batch_idx: 107
batch_idx: 108
batch_idx: 109
batch_idx: 110
Epoch [11/10], Step [111/944], Loss: 0.3620
batch_idx: 111
batch_idx: 112
batch_idx: 113
batch_idx: 114
batch_idx: 115
batch_idx: 116
batch_idx: 117
batch_idx: 118
batch_idx: 119
batch_idx: 120
Epoch [11/10], Step [121/944], Loss: 0.4026
batch_idx: 121
batch_idx: 122
batch_idx: 123
batch_idx: 124
batch_idx: 125
batch_idx: 126
batch_idx: 127
batch_idx: 128
batch_idx: 129
batch_idx: 130
Epoch [11/10], Step [131/944], Loss: 0.3646
batch_idx: 131
batch_idx: 132
batch_idx: 133
batch_idx: 134
batch_idx: 135
batch_idx: 136
batch_idx: 137
batch_idx: 138
batch_idx: 139
batch_idx: 140
Epoch [11/10], Step [141/944], Loss: 0.3897
batch_idx: 141
batch_idx: 142
batch_idx: 143
batch_idx: 144
batch_idx: 145
batch_idx: 146
batch_idx: 147
batch_idx: 148
batch_idx: 149
batch_idx: 150
Epoch [11/10], Step [151/944], Loss: 0.2721
batch_idx: 151
batch_idx: 152
batch_idx: 153
batch_idx: 154
batch_idx: 155
batch_idx: 156
batch_idx: 157
batch_idx: 158
batch_idx: 159
batch_idx: 160
Epoch [11/10], Step [161/944], Loss: 0.4943
batch_idx: 161
batch_idx: 162
batch_idx: 163
batch_idx: 164
batch_idx: 165
batch_idx: 166
batch_idx: 167
batch_idx: 168
batch_idx: 169
batch_idx: 170
Epoch [11/10], Step [171/944], Loss: 0.5325
batch_idx: 171
batch_idx: 172
batch_idx: 173
batch_idx: 174
batch_idx: 175
batch_idx: 176
batch_idx: 177
batch_idx: 178
batch_idx: 179
batch_idx: 180
Epoch [11/10], Step [181/944], Loss: 0.4401
batch_idx: 181
batch_idx: 182
batch_idx: 183
batch_idx: 184
batch_idx: 185
batch_idx: 186
batch_idx: 187
batch_idx: 188
batch_idx: 189
batch_idx: 190
Epoch [11/10], Step [191/944], Loss: 0.3707
batch_idx: 191
batch_idx: 192
batch_idx: 193
batch_idx: 194
batch_idx: 195
batch_idx: 196
batch_idx: 197
batch_idx: 198
batch_idx: 199
batch_idx: 200
Epoch [11/10], Step [201/944], Loss: 0.3343
batch_idx: 201
batch_idx: 202
batch_idx: 203
batch_idx: 204
batch_idx: 205
batch_idx: 206
batch_idx: 207
batch_idx: 208
batch_idx: 209
batch_idx: 210
Epoch [11/10], Step [211/944], Loss: 0.5143
batch_idx: 211
batch_idx: 212
batch_idx: 213
batch_idx: 214
batch_idx: 215
batch_idx: 216
batch_idx: 217
batch_idx: 218
batch_idx: 219
batch_idx: 220
Epoch [11/10], Step [221/944], Loss: 0.4606
batch_idx: 221
batch_idx: 222
batch_idx: 223
batch_idx: 224
batch_idx: 225
batch_idx: 226
batch_idx: 227
batch_idx: 228
batch_idx: 229
batch_idx: 230
Epoch [11/10], Step [231/944], Loss: 0.3976
batch_idx: 231
batch_idx: 232
batch_idx: 233
batch_idx: 234
batch_idx: 235
batch_idx: 236
batch_idx: 237
batch_idx: 238
batch_idx: 239
batch_idx: 240
Epoch [11/10], Step [241/944], Loss: 0.4944
batch_idx: 241
batch_idx: 242
batch_idx: 243
batch_idx: 244
batch_idx: 245
batch_idx: 246
batch_idx: 247
batch_idx: 248
batch_idx: 249
batch_idx: 250
Epoch [11/10], Step [251/944], Loss: 0.3399
batch_idx: 251
batch_idx: 252
batch_idx: 253
batch_idx: 254
batch_idx: 255
batch_idx: 256
batch_idx: 257
batch_idx: 258
batch_idx: 259
batch_idx: 260
Epoch [11/10], Step [261/944], Loss: 0.3320
batch_idx: 261
batch_idx: 262
batch_idx: 263
batch_idx: 264
batch_idx: 265
batch_idx: 266
batch_idx: 267
batch_idx: 268
batch_idx: 269
batch_idx: 270
Epoch [11/10], Step [271/944], Loss: 0.3412
batch_idx: 271
batch_idx: 272
batch_idx: 273
batch_idx: 274
batch_idx: 275
batch_idx: 276
batch_idx: 277
batch_idx: 278
batch_idx: 279
batch_idx: 280
Epoch [11/10], Step [281/944], Loss: 0.5285
batch_idx: 281
batch_idx: 282
batch_idx: 283
batch_idx: 284
batch_idx: 285
batch_idx: 286
batch_idx: 287
batch_idx: 288
batch_idx: 289
batch_idx: 290
Epoch [11/10], Step [291/944], Loss: 0.4180
batch_idx: 291
batch_idx: 292
batch_idx: 293
batch_idx: 294
batch_idx: 295
batch_idx: 296
batch_idx: 297
batch_idx: 298
batch_idx: 299
batch_idx: 300
Epoch [11/10], Step [301/944], Loss: 0.4498
batch_idx: 301
batch_idx: 302
batch_idx: 303
batch_idx: 304
batch_idx: 305
batch_idx: 306
batch_idx: 307
batch_idx: 308
batch_idx: 309
batch_idx: 310
Epoch [11/10], Step [311/944], Loss: 0.4522
batch_idx: 311
batch_idx: 312
batch_idx: 313
batch_idx: 314
batch_idx: 315
batch_idx: 316
batch_idx: 317
batch_idx: 318
batch_idx: 319
batch_idx: 320
Epoch [11/10], Step [321/944], Loss: 0.3504
batch_idx: 321
batch_idx: 322
batch_idx: 323
batch_idx: 324
batch_idx: 325
batch_idx: 326
batch_idx: 327
batch_idx: 328
batch_idx: 329
batch_idx: 330
Epoch [11/10], Step [331/944], Loss: 0.3946
batch_idx: 331
batch_idx: 332
batch_idx: 333
batch_idx: 334
batch_idx: 335
batch_idx: 336
batch_idx: 337
batch_idx: 338
batch_idx: 339
batch_idx: 340
Epoch [11/10], Step [341/944], Loss: 0.3704
batch_idx: 341
batch_idx: 342
batch_idx: 343
batch_idx: 344
batch_idx: 345
batch_idx: 346
batch_idx: 347
batch_idx: 348
batch_idx: 349
batch_idx: 350
Epoch [11/10], Step [351/944], Loss: 0.6361
batch_idx: 351
batch_idx: 352
batch_idx: 353
batch_idx: 354
batch_idx: 355
batch_idx: 356
batch_idx: 357
batch_idx: 358
batch_idx: 359
batch_idx: 360
Epoch [11/10], Step [361/944], Loss: 0.4212
batch_idx: 361
batch_idx: 362
batch_idx: 363
batch_idx: 364
batch_idx: 365
batch_idx: 366
batch_idx: 367
batch_idx: 368
batch_idx: 369
batch_idx: 370
Epoch [11/10], Step [371/944], Loss: 0.4459
batch_idx: 371
batch_idx: 372
batch_idx: 373
batch_idx: 374
batch_idx: 375
batch_idx: 376
batch_idx: 377
batch_idx: 378
batch_idx: 379
batch_idx: 380
Epoch [11/10], Step [381/944], Loss: 0.4815
batch_idx: 381
batch_idx: 382
batch_idx: 383
batch_idx: 384
batch_idx: 385
batch_idx: 386
batch_idx: 387
batch_idx: 388
batch_idx: 389
batch_idx: 390
Epoch [11/10], Step [391/944], Loss: 0.4670
batch_idx: 391
batch_idx: 392
batch_idx: 393
batch_idx: 394
batch_idx: 395
batch_idx: 396
batch_idx: 397
batch_idx: 398
batch_idx: 399
batch_idx: 400
Epoch [11/10], Step [401/944], Loss: 0.4654
batch_idx: 401
batch_idx: 402
batch_idx: 403
batch_idx: 404
batch_idx: 405
batch_idx: 406
batch_idx: 407
batch_idx: 408
batch_idx: 409
batch_idx: 410
Epoch [11/10], Step [411/944], Loss: 0.3476
batch_idx: 411
batch_idx: 412
batch_idx: 413
batch_idx: 414
batch_idx: 415
batch_idx: 416
batch_idx: 417
batch_idx: 418
batch_idx: 419
batch_idx: 420
Epoch [11/10], Step [421/944], Loss: 0.4462
batch_idx: 421
batch_idx: 422
batch_idx: 423
batch_idx: 424
batch_idx: 425
batch_idx: 426
batch_idx: 427
batch_idx: 428
batch_idx: 429
batch_idx: 430
Epoch [11/10], Step [431/944], Loss: 0.3448
batch_idx: 431
batch_idx: 432
batch_idx: 433
batch_idx: 434
batch_idx: 435
batch_idx: 436
batch_idx: 437
batch_idx: 438
batch_idx: 439
batch_idx: 440
Epoch [11/10], Step [441/944], Loss: 0.4123
batch_idx: 441
batch_idx: 442
batch_idx: 443
batch_idx: 444
batch_idx: 445
batch_idx: 446
batch_idx: 447
batch_idx: 448
batch_idx: 449
batch_idx: 450
Epoch [11/10], Step [451/944], Loss: 0.4973
batch_idx: 451
batch_idx: 452
batch_idx: 453
batch_idx: 454
batch_idx: 455
batch_idx: 456
batch_idx: 457
batch_idx: 458
batch_idx: 459
batch_idx: 460
Epoch [11/10], Step [461/944], Loss: 0.2770
batch_idx: 461
batch_idx: 462
batch_idx: 463
batch_idx: 464
batch_idx: 465
batch_idx: 466
batch_idx: 467
batch_idx: 468
batch_idx: 469
batch_idx: 470
Epoch [11/10], Step [471/944], Loss: 0.5414
batch_idx: 471
batch_idx: 472
batch_idx: 473
batch_idx: 474
batch_idx: 475
batch_idx: 476
batch_idx: 477
batch_idx: 478
batch_idx: 479
batch_idx: 480
Epoch [11/10], Step [481/944], Loss: 0.4387
batch_idx: 481
batch_idx: 482
batch_idx: 483
batch_idx: 484
batch_idx: 485
batch_idx: 486
batch_idx: 487
batch_idx: 488
batch_idx: 489
batch_idx: 490
Epoch [11/10], Step [491/944], Loss: 0.4254
batch_idx: 491
batch_idx: 492
batch_idx: 493
batch_idx: 494
batch_idx: 495
batch_idx: 496
batch_idx: 497
batch_idx: 498
batch_idx: 499
batch_idx: 500
Epoch [11/10], Step [501/944], Loss: 0.3940
batch_idx: 501
batch_idx: 502
batch_idx: 503
batch_idx: 504
batch_idx: 505
batch_idx: 506
batch_idx: 507
batch_idx: 508
batch_idx: 509
batch_idx: 510
Epoch [11/10], Step [511/944], Loss: 0.4936
batch_idx: 511
batch_idx: 512
batch_idx: 513
batch_idx: 514
batch_idx: 515
batch_idx: 516
batch_idx: 517
batch_idx: 518
batch_idx: 519
batch_idx: 520
Epoch [11/10], Step [521/944], Loss: 0.4248
batch_idx: 521
batch_idx: 522
batch_idx: 523
batch_idx: 524
batch_idx: 525
batch_idx: 526
batch_idx: 527
batch_idx: 528
batch_idx: 529
batch_idx: 530
Epoch [11/10], Step [531/944], Loss: 0.2375
batch_idx: 531
batch_idx: 532
batch_idx: 533
batch_idx: 534
batch_idx: 535
batch_idx: 536
batch_idx: 537
batch_idx: 538
batch_idx: 539
batch_idx: 540
Epoch [11/10], Step [541/944], Loss: 0.3564
batch_idx: 541
batch_idx: 542
batch_idx: 543
batch_idx: 544
batch_idx: 545
batch_idx: 546
batch_idx: 547
batch_idx: 548
batch_idx: 549
batch_idx: 550
Epoch [11/10], Step [551/944], Loss: 0.4098
batch_idx: 551
batch_idx: 552
batch_idx: 553
batch_idx: 554
batch_idx: 555
batch_idx: 556
batch_idx: 557
batch_idx: 558
batch_idx: 559
batch_idx: 560
Epoch [11/10], Step [561/944], Loss: 0.4067
batch_idx: 561
batch_idx: 562
batch_idx: 563
batch_idx: 564
batch_idx: 565
batch_idx: 566
batch_idx: 567
batch_idx: 568
batch_idx: 569
batch_idx: 570
Epoch [11/10], Step [571/944], Loss: 0.3434
batch_idx: 571
batch_idx: 572
batch_idx: 573
batch_idx: 574
batch_idx: 575
batch_idx: 576
batch_idx: 577
batch_idx: 578
batch_idx: 579
batch_idx: 580
Epoch [11/10], Step [581/944], Loss: 0.3424
batch_idx: 581
batch_idx: 582
batch_idx: 583
batch_idx: 584
batch_idx: 585
batch_idx: 586
batch_idx: 587
batch_idx: 588
batch_idx: 589
batch_idx: 590
Epoch [11/10], Step [591/944], Loss: 0.5038
batch_idx: 591
batch_idx: 592
batch_idx: 593
batch_idx: 594
batch_idx: 595
batch_idx: 596
batch_idx: 597
batch_idx: 598
batch_idx: 599
batch_idx: 600
Epoch [11/10], Step [601/944], Loss: 0.3996
batch_idx: 601
batch_idx: 602
batch_idx: 603
batch_idx: 604
batch_idx: 605
batch_idx: 606
batch_idx: 607
batch_idx: 608
batch_idx: 609
batch_idx: 610
Epoch [11/10], Step [611/944], Loss: 0.3502
batch_idx: 611
batch_idx: 612
batch_idx: 613
batch_idx: 614
batch_idx: 615
batch_idx: 616
batch_idx: 617
batch_idx: 618
batch_idx: 619
batch_idx: 620
Epoch [11/10], Step [621/944], Loss: 0.5129
batch_idx: 621
batch_idx: 622
batch_idx: 623
batch_idx: 624
batch_idx: 625
batch_idx: 626
batch_idx: 627
batch_idx: 628
batch_idx: 629
batch_idx: 630
Epoch [11/10], Step [631/944], Loss: 0.4100
batch_idx: 631
batch_idx: 632
batch_idx: 633
batch_idx: 634
batch_idx: 635
batch_idx: 636
batch_idx: 637
batch_idx: 638
batch_idx: 639
batch_idx: 640
Epoch [11/10], Step [641/944], Loss: 0.2595
batch_idx: 641
batch_idx: 642
batch_idx: 643
batch_idx: 644
batch_idx: 645
batch_idx: 646
batch_idx: 647
batch_idx: 648
batch_idx: 649
batch_idx: 650
Epoch [11/10], Step [651/944], Loss: 0.2811
batch_idx: 651
batch_idx: 652
batch_idx: 653
batch_idx: 654
batch_idx: 655
batch_idx: 656
batch_idx: 657
batch_idx: 658
batch_idx: 659
batch_idx: 660
Epoch [11/10], Step [661/944], Loss: 0.3725
batch_idx: 661
batch_idx: 662
batch_idx: 663
batch_idx: 664
batch_idx: 665
batch_idx: 666
batch_idx: 667
batch_idx: 668
batch_idx: 669
batch_idx: 670
Epoch [11/10], Step [671/944], Loss: 0.4149
batch_idx: 671
batch_idx: 672
batch_idx: 673
batch_idx: 674
batch_idx: 675
batch_idx: 676
batch_idx: 677
batch_idx: 678
batch_idx: 679
batch_idx: 680
Epoch [11/10], Step [681/944], Loss: 0.3444
batch_idx: 681
batch_idx: 682
batch_idx: 683
batch_idx: 684
batch_idx: 685
batch_idx: 686
batch_idx: 687
batch_idx: 688
batch_idx: 689
batch_idx: 690
Epoch [11/10], Step [691/944], Loss: 0.3758
batch_idx: 691
batch_idx: 692
batch_idx: 693
batch_idx: 694
batch_idx: 695
batch_idx: 696
batch_idx: 697
batch_idx: 698
batch_idx: 699
batch_idx: 700
Epoch [11/10], Step [701/944], Loss: 0.3641
batch_idx: 701
batch_idx: 702
batch_idx: 703
batch_idx: 704
batch_idx: 705
batch_idx: 706
batch_idx: 707
batch_idx: 708
batch_idx: 709
batch_idx: 710
Epoch [11/10], Step [711/944], Loss: 0.4627
batch_idx: 711
batch_idx: 712
batch_idx: 713
batch_idx: 714
batch_idx: 715
batch_idx: 716
batch_idx: 717
batch_idx: 718
batch_idx: 719
batch_idx: 720
Epoch [11/10], Step [721/944], Loss: 0.3558
batch_idx: 721
batch_idx: 722
batch_idx: 723
batch_idx: 724
batch_idx: 725
batch_idx: 726
batch_idx: 727
batch_idx: 728
batch_idx: 729
batch_idx: 730
Epoch [11/10], Step [731/944], Loss: 0.5057
batch_idx: 731
batch_idx: 732
batch_idx: 733
batch_idx: 734
batch_idx: 735
batch_idx: 736
batch_idx: 737
batch_idx: 738
batch_idx: 739
batch_idx: 740
Epoch [11/10], Step [741/944], Loss: 0.5337
batch_idx: 741
batch_idx: 742
batch_idx: 743
batch_idx: 744
batch_idx: 745
batch_idx: 746
batch_idx: 747
batch_idx: 748
batch_idx: 749
batch_idx: 750
Epoch [11/10], Step [751/944], Loss: 0.4165
batch_idx: 751
batch_idx: 752
batch_idx: 753
batch_idx: 754
batch_idx: 755
batch_idx: 756
batch_idx: 757
batch_idx: 758
batch_idx: 759
batch_idx: 760
Epoch [11/10], Step [761/944], Loss: 0.4412
batch_idx: 761
batch_idx: 762
batch_idx: 763
batch_idx: 764
batch_idx: 765
batch_idx: 766
batch_idx: 767
batch_idx: 768
batch_idx: 769
batch_idx: 770
Epoch [11/10], Step [771/944], Loss: 0.3679
batch_idx: 771
batch_idx: 772
batch_idx: 773
batch_idx: 774
batch_idx: 775
batch_idx: 776
batch_idx: 777
batch_idx: 778
batch_idx: 779
batch_idx: 780
Epoch [11/10], Step [781/944], Loss: 0.4029
batch_idx: 781
batch_idx: 782
batch_idx: 783
batch_idx: 784
batch_idx: 785
batch_idx: 786
batch_idx: 787
batch_idx: 788
batch_idx: 789
batch_idx: 790
Epoch [11/10], Step [791/944], Loss: 0.4127
batch_idx: 791
batch_idx: 792
batch_idx: 793
batch_idx: 794
batch_idx: 795
batch_idx: 796
batch_idx: 797
batch_idx: 798
batch_idx: 799
batch_idx: 800
Epoch [11/10], Step [801/944], Loss: 0.3353
batch_idx: 801
batch_idx: 802
batch_idx: 803
batch_idx: 804
batch_idx: 805
batch_idx: 806
batch_idx: 807
batch_idx: 808
batch_idx: 809
batch_idx: 810
Epoch [11/10], Step [811/944], Loss: 0.2518
batch_idx: 811
batch_idx: 812
batch_idx: 813
batch_idx: 814
batch_idx: 815
batch_idx: 816
batch_idx: 817
batch_idx: 818
batch_idx: 819
batch_idx: 820
Epoch [11/10], Step [821/944], Loss: 0.6384
batch_idx: 821
batch_idx: 822
batch_idx: 823
batch_idx: 824
batch_idx: 825
batch_idx: 826
batch_idx: 827
batch_idx: 828
batch_idx: 829
batch_idx: 830
Epoch [11/10], Step [831/944], Loss: 0.4496
batch_idx: 831
batch_idx: 832
batch_idx: 833
batch_idx: 834
batch_idx: 835
batch_idx: 836
batch_idx: 837
batch_idx: 838
batch_idx: 839
batch_idx: 840
Epoch [11/10], Step [841/944], Loss: 0.3354
batch_idx: 841
batch_idx: 842
batch_idx: 843
batch_idx: 844
batch_idx: 845
batch_idx: 846
batch_idx: 847
batch_idx: 848
batch_idx: 849
batch_idx: 850
Epoch [11/10], Step [851/944], Loss: 0.4292
batch_idx: 851
batch_idx: 852
batch_idx: 853
batch_idx: 854
batch_idx: 855
batch_idx: 856
batch_idx: 857
batch_idx: 858
batch_idx: 859
batch_idx: 860
Epoch [11/10], Step [861/944], Loss: 0.5524
batch_idx: 861
batch_idx: 862
batch_idx: 863
batch_idx: 864
batch_idx: 865
batch_idx: 866
batch_idx: 867
batch_idx: 868
batch_idx: 869
batch_idx: 870
Epoch [11/10], Step [871/944], Loss: 0.3566
batch_idx: 871
batch_idx: 872
batch_idx: 873
batch_idx: 874
batch_idx: 875
batch_idx: 876
batch_idx: 877
batch_idx: 878
batch_idx: 879
batch_idx: 880
Epoch [11/10], Step [881/944], Loss: 0.3098
batch_idx: 881
batch_idx: 882
batch_idx: 883
batch_idx: 884
batch_idx: 885
batch_idx: 886
batch_idx: 887
batch_idx: 888
batch_idx: 889
batch_idx: 890
Epoch [11/10], Step [891/944], Loss: 0.3757
batch_idx: 891
batch_idx: 892
batch_idx: 893
batch_idx: 894
batch_idx: 895
batch_idx: 896
batch_idx: 897
batch_idx: 898
batch_idx: 899
batch_idx: 900
Epoch [11/10], Step [901/944], Loss: 0.3844
batch_idx: 901
batch_idx: 902
batch_idx: 903
batch_idx: 904
batch_idx: 905
batch_idx: 906
batch_idx: 907
batch_idx: 908
batch_idx: 909
batch_idx: 910
Epoch [11/10], Step [911/944], Loss: 0.4800
batch_idx: 911
batch_idx: 912
batch_idx: 913
batch_idx: 914
batch_idx: 915
batch_idx: 916
batch_idx: 917
batch_idx: 918
batch_idx: 919
batch_idx: 920
Epoch [11/10], Step [921/944], Loss: 0.4616
batch_idx: 921
batch_idx: 922
batch_idx: 923
batch_idx: 924
batch_idx: 925
batch_idx: 926
batch_idx: 927
batch_idx: 928
batch_idx: 929
batch_idx: 930
Epoch [11/10], Step [931/944], Loss: 0.5083
batch_idx: 931
batch_idx: 932
batch_idx: 933
batch_idx: 934
batch_idx: 935
batch_idx: 936
batch_idx: 937
batch_idx: 938
batch_idx: 939
batch_idx: 940
Epoch [11/10], Step [941/944], Loss: 0.2638
batch_idx: 941
batch_idx: 942
batch_idx: 943
Gradients for clr_model.model.roberta.embeddings.word_embeddings.weight: None
Gradients for clr_model.model.roberta.embeddings.position_embeddings.weight: None
Gradients for clr_model.model.roberta.embeddings.token_type_embeddings.weight: None
Gradients for clr_model.model.roberta.embeddings.LayerNorm.weight: None
Gradients for clr_model.model.roberta.embeddings.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.0.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.1.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.2.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.3.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.4.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.5.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.6.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.7.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.8.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.9.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.10.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.query.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.query.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.key.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.key.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.value.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.self.value.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.attention.output.LayerNorm.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.intermediate.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.intermediate.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.dense.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.dense.bias: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.LayerNorm.weight: None
Gradients for clr_model.model.roberta.encoder.layer.11.output.LayerNorm.bias: None
Gradients for clr_model.model.classifier.dense.weight: None
Gradients for clr_model.model.classifier.dense.bias: None
Gradients for clr_model.model.classifier.out_proj.weight: None
Gradients for clr_model.model.classifier.out_proj.bias: None
Gradients for devign_model.ggnn.linears.0.weight: 0.0
Gradients for devign_model.ggnn.linears.0.bias: 0.0
Gradients for devign_model.ggnn.linears.1.weight: 0.0
Gradients for devign_model.ggnn.linears.1.bias: 0.0
Gradients for devign_model.ggnn.linears.2.weight: 0.0
Gradients for devign_model.ggnn.linears.2.bias: 0.0
Gradients for devign_model.ggnn.linears.3.weight: 0.0
Gradients for devign_model.ggnn.linears.3.bias: 0.0
Gradients for devign_model.ggnn.linears.4.weight: 0.0
Gradients for devign_model.ggnn.linears.4.bias: 0.0
Gradients for devign_model.ggnn.linears.5.weight: 0.0
Gradients for devign_model.ggnn.linears.5.bias: 0.0
Gradients for devign_model.ggnn.gru.weight_ih: 0.0
Gradients for devign_model.ggnn.gru.weight_hh: 0.0
Gradients for devign_model.ggnn.gru.bias_ih: 0.0
Gradients for devign_model.ggnn.gru.bias_hh: 0.0
Gradients for devign_model.conv_l1.weight: 0.0
Gradients for devign_model.conv_l1.bias: 0.0
Gradients for devign_model.conv_l2.weight: 0.0
Gradients for devign_model.conv_l2.bias: 0.0
Gradients for devign_model.conv_l1_for_concat.weight: 0.0
Gradients for devign_model.conv_l1_for_concat.bias: 0.0
Gradients for devign_model.conv_l2_for_concat.weight: 0.0
Gradients for devign_model.conv_l2_for_concat.bias: 0.0
Gradients for devign_model.mlp_z.weight: 0.0
Gradients for devign_model.mlp_z.bias: 0.0
Gradients for devign_model.mlp_y.weight: 0.0
Gradients for devign_model.mlp_y.bias: 0.0
Gradients for classifier.weight: 4.470348358154297e-08
Gradients for classifier.bias: 0.0
Epoch 11/10, Average Loss: 0.4013
Model saved to /home/ngan/Documents/SamVulDetection/saved_models/model_epoch_11.pth

